{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"examples/bridged-network/","text":"Setting up nodes over bridged network This example shows how to configure a simple bridge interface using netplan . Step 1 - (Pre)configure the bridge on the host In order to use the bridged network, bridge interface needs to be preconfigured on the host machine. Create the bridge interface ( br0 in our case) by creating a file with the following content: /etc/netplan/bridge0.yaml network : version : 2 renderer : networkd ethernets : eth0 : {} # (1) bridges : br0 : # (2) interfaces : - eth0 dhcp4 : true dhcp6 : false addresses : # (3) - 10.10.0.17 Existing ethernet interface to be enslaved. Custom name of the bridge interface. Optionally a static IP address can be set for the bridge interface. Tip See the official netplan configuration examples for more complex configurations. Validate if the configuration is correctly parsed by netplan. sudo netplan generate Apply the configuration. sudo netplan apply Step 2 - Disable netfilter on the host The final step is to prevent packets traversing the bridge from being sent to iptables for processing. cat >> /etc/sysctl.conf <<EOF net.bridge.bridge-nf-call-ip6tables = 0 net.bridge.bridge-nf-call-iptables = 0 net.bridge.bridge-nf-call-arptables = 0 EOF sysctl -p /etc/sysctl.conf Tip For more information, see the libvirt documentation . Step 3 - Set up a cluster over bridged network In the cluster configuration file, set the following variables: cluster.network.mode to bridge , cluster.network.bridge to the name of the bridge you have created ( br0 in our case) and cluster.network.gateway if the first host in network_cidr is not a gateway. cluster : network : mode : \"bridge\" cidr : \"10.10.13.0/24\" gateway : \"10.10.13.1\" bridge : \"br0\" ...","title":"Bridged network"},{"location":"examples/bridged-network/#step-1-preconfigure-the-bridge-on-the-host","text":"In order to use the bridged network, bridge interface needs to be preconfigured on the host machine. Create the bridge interface ( br0 in our case) by creating a file with the following content: /etc/netplan/bridge0.yaml network : version : 2 renderer : networkd ethernets : eth0 : {} # (1) bridges : br0 : # (2) interfaces : - eth0 dhcp4 : true dhcp6 : false addresses : # (3) - 10.10.0.17 Existing ethernet interface to be enslaved. Custom name of the bridge interface. Optionally a static IP address can be set for the bridge interface. Tip See the official netplan configuration examples for more complex configurations. Validate if the configuration is correctly parsed by netplan. sudo netplan generate Apply the configuration. sudo netplan apply","title":"Step 1 - (Pre)configure the bridge on the host"},{"location":"examples/bridged-network/#step-2-disable-netfilter-on-the-host","text":"The final step is to prevent packets traversing the bridge from being sent to iptables for processing. cat >> /etc/sysctl.conf <<EOF net.bridge.bridge-nf-call-ip6tables = 0 net.bridge.bridge-nf-call-iptables = 0 net.bridge.bridge-nf-call-arptables = 0 EOF sysctl -p /etc/sysctl.conf Tip For more information, see the libvirt documentation .","title":"Step 2 - Disable netfilter on the host"},{"location":"examples/bridged-network/#step-3-set-up-a-cluster-over-bridged-network","text":"In the cluster configuration file, set the following variables: cluster.network.mode to bridge , cluster.network.bridge to the name of the bridge you have created ( br0 in our case) and cluster.network.gateway if the first host in network_cidr is not a gateway. cluster : network : mode : \"bridge\" cidr : \"10.10.13.0/24\" gateway : \"10.10.13.1\" bridge : \"br0\" ...","title":"Step 3 - Set up a cluster over bridged network"},{"location":"examples/full-example/","text":"Full (detailed) example # # In the 'kubitect' section, you can specify the target git project and version. # This can be handy if you want to use a specific project version or if you # want to point to your forked/cloned project. # # [!] Note that this is ignored if you use the --local option with the # actions of the CLI tool, since in this case you should be in the # Git repository. # kubitect : url : \"https://github.com/MusicDin/kubitect\" # (1) version : \"v2.0.7\" # # The \"hosts\" section contains data about the physical servers on which # the Kubernetes cluster will be installed. # # For each host, a name and connection type must be specified. Only one # host can have the connection type set to 'local' or 'localhost'. # # If the host is a remote machine, SSH key file must be specified. # [!] Note that the connection to the remote hosts supports only # passwordless login (using only SSH keyfile). # # The host can also be marked as default, i.e. if no specific host is # specified for an instance (in the cluster.nodes section), it will be # installed on a default host. If none of the hosts is marked as default, # the first one in the list is used as default host. # hosts : - name : localhost # (3) default : true # (4) connection : type : local # (5) - name : remote-server-1 connection : type : remote user : myuser # (6) ip : 10.10.40.143 # (7) ssh : port : 1234 # (8) verify : false # (9) keyfile : \"~/.ssh/id_rsa_server1\" # (10) - name : remote-server-2 connection : type : remote user : myuser ip : 10.10.40.144 ssh : keyfile : \"~/.ssh/id_rsa_server2\" mainResourcePoolPath : \"/var/lib/libvirt/pools/\" # (11) dataResourcePools : # (12) - name : data-pool path : \"/mnt/data/pool\" - name : backup-pool path : \"/mnt/backup/pool\" # # The \"cluster\" section of configuration contains general data about the cluster, # nodes that are part of the cluster and cluster's network. # cluster : name : \"my-k8s-cluster\" # (13) network : mode : bridge # (14) cidr : \"10.10.64.0/24\" # (15) gateway : 10.10.64.1 # (16) bridge : br0 # (17) dns : # (18) - 1.1.1.1 - 1.0.0.1 nodeTemplate : networkInterface : \"ens3\" # (19) user : \"k8s\" ssh : privateKeyPath : \"~/.ssh/id_rsa_test\" addToKnownHosts : true image : distro : \"ubuntu\" source : \"https://cloud-images.ubuntu.com/releases/focal/release-20220111/ubuntu-20.04-server-cloudimg-amd64.img\" updateOnBoot : true nodes : loadBalancer : vip : \"10.10.64.200\" # (20) default : # (21) ram : 4 # GiB cpu : 1 # vCPU mainDiskSize : 16 # GiB instances : - id : 1 ip : 10.10.64.5 # (22) mac : \"52:54:00:00:00:40\" # (23) ram : 8 # (24) cpu : 8 # (25) host : remote-server-1 # (26) - id : 2 ip : 10.10.64.6 mac : \"52:54:00:00:00:41\" host : remote-server-2 - id : 3 ip : 10.10.64.7 mac : \"52:54:00:00:00:42\" # If host is not specifed, VM will be installed on the default host. # If default host is not specified, VM will be installed on the first # host in the list. master : default : ram : 8 cpu : 2 mainDiskSize : 256 instances : # IMPORTANT: There should be odd number of master nodes. - id : 1 # Node with generated MAC address, IP retrieved as an DHCP lease and default RAM and CPU. host : remote-server-1 - id : 2 host : remote-server-2 - id : 3 host : localhost worker : default : ram : 16 cpu : 4 label : node # (27) # Default dataDisks are NOT YET supported # dataDisks: # (29) # - name: rook-disk # (30) # pool: data-pool # (31) # size: 128 # (32) # - name: backup-disk # pool: data-pool # size: 512 instances : - id : 1 ip : 10.10.64.101 cpu : 8 ram : 64 host : remote-server-1 - id : 2 ip : 10.10.64.102 dataDisks : # (33) - name : rook-disk pool : data-pool size : 128 - name : test-disk pool : data-pool size : 128 - id : 3 ip : 10.10.64.103 ram : 64 - id : 4 host : remote-server-2 - id : 5 # # The \"kubernetes\" section specifies which version of Kubernetes and # Kubespray to use and which network plugin and DNS server to install. # kubernetes : version : \"v1.22.6\" networkPlugin : calico dnsMode : coredns kubespray : url : \"https://github.com/kubernetes-sigs/kubespray.git\" version : \"v2.18.1\" other : copyKubeconfig : false This allows you to set a custom URL that targets clone/fork of Kubitect project. Kubitect version. Custom host name. It is used to link instances to the specific host. Makes the host a default host. This means that if no host is specified for the node instance, the instance will be linked to the default host. Connection type can be either local or remote . If it is set to remote , at least the following fields must be set: user ip ssh.keyfile Remote host user that is used to connect to the remote hypervisor. This user must be added in the libvirt group. IP address of the remote host. Overrides default SSH port (22). If set to false, host verification is skipped. Path to the passwordless SSH key used to connect to the remote host. The path to the main resource pool defines where the virtual machine disk images are stored. These disks contain the virtual machine operating system, and therefore it is recommended to install them on SSD disks. List of other storage pools where virtual disks can be created. Cluster name used as a prefix for the various components. Network mode. Possible values are bridge mode uses predefined bridge interface. This mode is mandatory for deployments across multiple hosts. nat mode creates virtual network with IP range defined in network.cidr route Network CIDR represents the network IP together with the network mask. In nat mode, CIDR is used for the new network. In bridge mode, CIDR represents the current local area network (LAN). The network gateway IP address. If omitted the first client IP from network CIDR is used as a gateway. Bridge represents the bridge interface on the hosts. This field is mandatory if the network mode is set to bridge . If the network mode is set to nat , this field can be omitted. Set custom DNS for nodes. If omitted, gateway is also used as DNS. Specify the network interface used by the virtual machine. In general, this option can be omitted. If you omit it, ens3 is used for Ubuntu images and eth0 for all other distributions. Virtual (floating) IP shared between load balancers. Default values apply for all virtual machines (VMs) of the same type. Static IP address of the virtual machine. If omitted DHCP lease is requested. Static MAC address. If omitted MAC address is generated. Overrides default RAM value for this node. Overrides default CPU value for this node. Name of the host where instance should be created. If omitted the default host is used. Worker nodes label. Overrides default data disks for this node. Default data disks (attached to each worker node). Unique data disk name. Reference to the data resource pool that must exist on the same host as this node. Size of the data disk in GiB. Note that each node receives a data disk of a specific size. Overrides default data disks for this node.","title":"Full example"},{"location":"examples/single-node-cluster/","text":"Single node cluster If you want to initialize a cluster with only one node, specify single master node in cluster configuration file: single-node.yaml cluster : ... nodes : master : instances : - id : 1 ip : 10.10.64.5 # (1) mac : \"52:54:00:00:00:40\" # (2) Static IP address. If omitted, the DHCP lease is requested. Static MAC address. If omitted, the MAC address is generated. Do not forget to remove (or comment out) the worker and load balancer nodes. Apply the cluster: kubitect apply --config single-node.yaml Your master node now also becomes a worker node. Note If you do not specify worker nodes, all master nodes also become worker nodes.","title":"Single node cluster"},{"location":"reference/reference/","text":"Reference The cluster configuration consists of four parts: kubitect - project metadata. hosts - a list of physical hosts (local or remote). cluster - cluster infrastructure configuration. Virtual machine properties, node types to install, and the host on which to install the nodes. kubernetes - Kubernetes and Kubespray configuration. Versions, addons and other Kubernetes related settings. Note [*] annotates an array. Kubitect section Name Type Default value Required? Description kubitect.url string https://github.com/MusicDin/kubitect No URL of the project's git repository. kubitect.version string master No Version of the git repository. Can be a branch or a tag. Hosts section Name Type Default value Required? Description hosts[*].name string Yes Custom server name used to link nodes with physical hosts. hosts[*].default string false Nodes where host is not specified will be installed on default host. The first host in the list is used as a default host if none is marked as a default. hosts[*].connection.type string Yes Possible values are: local or localhost remote hosts[*].connection.user string Yes, if connection.type is set to remote Username is used to SSH into the remote machine. hosts[*].connection.ip string Yes, if connection.type is set to remote IP address is used to SSH into the remote machine. hosts[*].connection.ssh.port number 22 The port number of SSH protocol for remote machine. hosts[*].connection.ssh.keyfile string ~/.ssh/id_rsa Path to the keyfile that is used to SSH into the remote machine hosts[*].connection.ssh.verify boolean true If set to true, SSH host is verified. hosts[*].mainResourcePoolPath string /var/lib/libvirt/pools/ Path to the resource pool used for main virtual machine volumes. hosts[*].dataResourcePools[*].name string Name of the data resource pool. Must be unique within the same host. It is used to link virtual machine volumes to the specific resource pool. hosts[*].dataResourcePools[*].path string Host path to the location where data resource pool is created. Cluster section Name Type Default value Required? Description cluster.name string Yes Custom cluster name that is used as a prefix for various cluster components. cluster.network.mode string nat Yes Network mode. Possible values are: nat - Creates virtual local network. bridge - Uses preconfigured bridge interface on the machine (Only bridge mode supports multiple hosts). route - Creates virtual local network, but does not apply NAT. cluster.network.cidr string Yes Network cidr that contains network IP with network mask bits (IPv4/mask_bits). cluster.network.gateway string First client IP in network. By default first client IP is taken as a gateway. If network cidr is set to 10.0.0.0/24 then gateway would be 10.0.0.1. Set gateway if it differs from default value. cluster.network.bridge string virbr0 By default virbr0 is set as a name of virtual bridge. In case network mode is set to bridge, name of the preconfigured bridge needs to be set here. cluster.network.dns list [ Network gateway ] DNS used by all created virtual machines. If none is provided, gateway is used as a DNS server. cluster.nodeTemplate.images.networkInterface string ens3, if distro is set to ubuntu, otherwise eth0 Network interface used by virtual machines to connect to the network. Usually for Ubuntu images is ens3 and for most other distros is eth0. cluster.nodeTemplate.user string user User created on each virtual machine. cluster.nodeTemplate.ssh.privateKeyPath string Path to private key that is later used to SSH into each virtual machine. On the same path with .pub prefix needs to be present public key. If this value is not set, SSH key will be generated in ./config/.ssh/ directory. cluster.nodeTemplate.ssh.addToKnownHosts boolean true If set to true, each virtual machine will be added to the known hosts on the machine where the project is being run. Note that all machines will also be removed from known hosts when destroying the cluster. cluster.nodeTemplate.image.distro string N/A Set OS image distribution. Possible values are: ubuntu debian centos N/A - For all other distros cluster.nodeTemplate.image.source string Yes Source of an OS image. It can be either path on a local file system or a URL to an image. cluster.nodeTemplate.updateOnBoot boolean true If set to true, the operating system will be updated when it boots. cluster.nodes.loadBalancer.vip string Yes, if more then one instance of load balancer is specified. Virtual IP (floating IP) is the static IP used by load balancers to provide a fail-over. Each load balancer still has its own IP beside the shared one. cluster.nodes.loadBalancer.default.ram number 4 Yes Default amount of RAM (in GiB) allocated to a load balancer instance. cluster.nodes.loadBalancer.default.cpu number 1 Default number of vCPU allocated to a load balancer instance. cluster.nodes.loadBalancer.default.mainDiskSize number 16 Size of the main disk (in GiB) that is attached to a load balancer instance. cluster.nodes.loadBalancer.instances[*].id number Yes Unique numeric identifier of a load balancer instance. It has to be a number between 0 and 200, because the priority of a load balancer is calculated out of its ID. cluster.nodes.loadBalancer.instances[*].ip string If an IP is set for an instance then the instance will use it as a static IP. Otherwise it will try to request an IP from a DHCP server. cluster.nodes.loadBalancer.instances[*].mac string MAC used by the instance. If it is not set, it will be generated. cluster.nodes.loadBalancer.instances[*].ram number Overrides a default value for the RAM for that instance. cluster.nodes.loadBalancer.instances[*].cpu number Overrides a default value for that specific instance. cluster.nodes.loadBalancer.instances[*].mainDiskSize number Overrides a default value for that specific instance. cluster.nodes.master.default.ram number 4 Default amount of RAM (in GiB) allocated to a master node. cluster.nodes.master.default.cpu number 1 Default number of vCPU allocated to a master node. cluster.nodes.master.default.mainDiskSize number 16 Size of the main disk (in GiB) that is attached to a master node. cluster.nodes.master.instances[*].id number Yes Unique numeric identifier of a master node. cluster.nodes.master.instances[*].ip string If an IP is set for an instance then the instance will use it as a static IP. Otherwise it will try to request an IP from a DHCP server. cluster.nodes.master.instances[*].mac string MAC used by the instance. If it is not set, it will be generated. cluster.nodes.master.instances[*].ram number Overrides a default value for the RAM for that instance. cluster.nodes.master.instances[*].cpu number Overrides a default value for that specific instance. cluster.nodes.master.instances[*].mainDiskSize number Overrides a default value for that specific instance. cluster.nodes.master.instances[*].dataDisks[*].name string Name of the additional data disk that is attached to the master node. cluster.nodes.master.instances[*].dataDisks[*].pool string Name of the data resource pool where the additional data disk is created. Referenced resource pool must be specified on the same host. cluster.nodes.master.instances[*].dataDisks[*].size string Size of the additional data disk (in GiB) that is attached to the master node. cluster.nodes.worker.default.ram number 8 Default amount of RAM (in GiB) allocated to a worker node. cluster.nodes.worker.default.cpu number 2 Default number of vCPU allocated to a worker node. cluster.nodes.worker.default.mainDiskSize number 32 Size of the main disk (in GiB) that is attached to a worker node. cluster.nodes.worker.default.label string Default label for worker nodes. cluster.nodes.worker.instances[*].id number Yes Unique numeric identifier of a worker node. cluster.nodes.worker.instances[*].ip string If an IP is set for an instance then the instance will use it as a static IP. Otherwise it will try to request an IP from a DHCP server. cluster.nodes.worker.instances[*].mac string MAC used by the instance. If it is not set, it will be generated. cluster.nodes.worker.instances[*].ram number Overrides a default value for the RAM for that instance. cluster.nodes.worker.instances[*].cpu number Overrides a default value for that specific instance. cluster.nodes.worker.instances[*].mainDiskSize number Overrides a default value for that specific instance. cluster.nodes.worker.instances[*].dataDisks[*].name string Name of the additional data disk that is attached to the worker node. cluster.nodes.worker.instances[*].dataDisks[*].pool string Name of the data resource pool where the additional data disk is created. Referenced resource pool must be specified on the same host. cluster.nodes.worker.instances[*].dataDisks[*].size string Size of the additional data disk (in GiB) that is attached to the worker node. Kubernetes section Name Type Default value Required? Description kubernetes.version string Yes Kubernetes version that will be installed. kubernetes.networkPlugin string calico Network plugin used within a Kubernetes cluster. Possible values are: flannel weave calico cilium canal kube-router kubernetes.dnsMode string coredns DNS server used within a Kubernetes cluster. Possible values are: coredns kubedns kubernetes.kubespray.url string https://github.com/kubernetes-sigs/kubespray.git URL to the Kubespray project. For example, it can be changed so that it targets your fork of a project. kubernetes.kubespray.version string Yes Kubespray version. Version is used to checkout into appropriate branch. kubernetes.kubespray.other.copyKubeconfig boolean false If set to true, Kubeconfig of a new cluster will be copied to the `~/.kube/admin.conf`. Please note that setting this to true can result in overwriting file on target location.","title":"Reference"},{"location":"reference/reference/#kubitect-section","text":"Name Type Default value Required? Description kubitect.url string https://github.com/MusicDin/kubitect No URL of the project's git repository. kubitect.version string master No Version of the git repository. Can be a branch or a tag.","title":"Kubitect section"},{"location":"reference/reference/#hosts-section","text":"Name Type Default value Required? Description hosts[*].name string Yes Custom server name used to link nodes with physical hosts. hosts[*].default string false Nodes where host is not specified will be installed on default host. The first host in the list is used as a default host if none is marked as a default. hosts[*].connection.type string Yes Possible values are: local or localhost remote hosts[*].connection.user string Yes, if connection.type is set to remote Username is used to SSH into the remote machine. hosts[*].connection.ip string Yes, if connection.type is set to remote IP address is used to SSH into the remote machine. hosts[*].connection.ssh.port number 22 The port number of SSH protocol for remote machine. hosts[*].connection.ssh.keyfile string ~/.ssh/id_rsa Path to the keyfile that is used to SSH into the remote machine hosts[*].connection.ssh.verify boolean true If set to true, SSH host is verified. hosts[*].mainResourcePoolPath string /var/lib/libvirt/pools/ Path to the resource pool used for main virtual machine volumes. hosts[*].dataResourcePools[*].name string Name of the data resource pool. Must be unique within the same host. It is used to link virtual machine volumes to the specific resource pool. hosts[*].dataResourcePools[*].path string Host path to the location where data resource pool is created.","title":"Hosts section"},{"location":"reference/reference/#cluster-section","text":"Name Type Default value Required? Description cluster.name string Yes Custom cluster name that is used as a prefix for various cluster components. cluster.network.mode string nat Yes Network mode. Possible values are: nat - Creates virtual local network. bridge - Uses preconfigured bridge interface on the machine (Only bridge mode supports multiple hosts). route - Creates virtual local network, but does not apply NAT. cluster.network.cidr string Yes Network cidr that contains network IP with network mask bits (IPv4/mask_bits). cluster.network.gateway string First client IP in network. By default first client IP is taken as a gateway. If network cidr is set to 10.0.0.0/24 then gateway would be 10.0.0.1. Set gateway if it differs from default value. cluster.network.bridge string virbr0 By default virbr0 is set as a name of virtual bridge. In case network mode is set to bridge, name of the preconfigured bridge needs to be set here. cluster.network.dns list [ Network gateway ] DNS used by all created virtual machines. If none is provided, gateway is used as a DNS server. cluster.nodeTemplate.images.networkInterface string ens3, if distro is set to ubuntu, otherwise eth0 Network interface used by virtual machines to connect to the network. Usually for Ubuntu images is ens3 and for most other distros is eth0. cluster.nodeTemplate.user string user User created on each virtual machine. cluster.nodeTemplate.ssh.privateKeyPath string Path to private key that is later used to SSH into each virtual machine. On the same path with .pub prefix needs to be present public key. If this value is not set, SSH key will be generated in ./config/.ssh/ directory. cluster.nodeTemplate.ssh.addToKnownHosts boolean true If set to true, each virtual machine will be added to the known hosts on the machine where the project is being run. Note that all machines will also be removed from known hosts when destroying the cluster. cluster.nodeTemplate.image.distro string N/A Set OS image distribution. Possible values are: ubuntu debian centos N/A - For all other distros cluster.nodeTemplate.image.source string Yes Source of an OS image. It can be either path on a local file system or a URL to an image. cluster.nodeTemplate.updateOnBoot boolean true If set to true, the operating system will be updated when it boots. cluster.nodes.loadBalancer.vip string Yes, if more then one instance of load balancer is specified. Virtual IP (floating IP) is the static IP used by load balancers to provide a fail-over. Each load balancer still has its own IP beside the shared one. cluster.nodes.loadBalancer.default.ram number 4 Yes Default amount of RAM (in GiB) allocated to a load balancer instance. cluster.nodes.loadBalancer.default.cpu number 1 Default number of vCPU allocated to a load balancer instance. cluster.nodes.loadBalancer.default.mainDiskSize number 16 Size of the main disk (in GiB) that is attached to a load balancer instance. cluster.nodes.loadBalancer.instances[*].id number Yes Unique numeric identifier of a load balancer instance. It has to be a number between 0 and 200, because the priority of a load balancer is calculated out of its ID. cluster.nodes.loadBalancer.instances[*].ip string If an IP is set for an instance then the instance will use it as a static IP. Otherwise it will try to request an IP from a DHCP server. cluster.nodes.loadBalancer.instances[*].mac string MAC used by the instance. If it is not set, it will be generated. cluster.nodes.loadBalancer.instances[*].ram number Overrides a default value for the RAM for that instance. cluster.nodes.loadBalancer.instances[*].cpu number Overrides a default value for that specific instance. cluster.nodes.loadBalancer.instances[*].mainDiskSize number Overrides a default value for that specific instance. cluster.nodes.master.default.ram number 4 Default amount of RAM (in GiB) allocated to a master node. cluster.nodes.master.default.cpu number 1 Default number of vCPU allocated to a master node. cluster.nodes.master.default.mainDiskSize number 16 Size of the main disk (in GiB) that is attached to a master node. cluster.nodes.master.instances[*].id number Yes Unique numeric identifier of a master node. cluster.nodes.master.instances[*].ip string If an IP is set for an instance then the instance will use it as a static IP. Otherwise it will try to request an IP from a DHCP server. cluster.nodes.master.instances[*].mac string MAC used by the instance. If it is not set, it will be generated. cluster.nodes.master.instances[*].ram number Overrides a default value for the RAM for that instance. cluster.nodes.master.instances[*].cpu number Overrides a default value for that specific instance. cluster.nodes.master.instances[*].mainDiskSize number Overrides a default value for that specific instance. cluster.nodes.master.instances[*].dataDisks[*].name string Name of the additional data disk that is attached to the master node. cluster.nodes.master.instances[*].dataDisks[*].pool string Name of the data resource pool where the additional data disk is created. Referenced resource pool must be specified on the same host. cluster.nodes.master.instances[*].dataDisks[*].size string Size of the additional data disk (in GiB) that is attached to the master node. cluster.nodes.worker.default.ram number 8 Default amount of RAM (in GiB) allocated to a worker node. cluster.nodes.worker.default.cpu number 2 Default number of vCPU allocated to a worker node. cluster.nodes.worker.default.mainDiskSize number 32 Size of the main disk (in GiB) that is attached to a worker node. cluster.nodes.worker.default.label string Default label for worker nodes. cluster.nodes.worker.instances[*].id number Yes Unique numeric identifier of a worker node. cluster.nodes.worker.instances[*].ip string If an IP is set for an instance then the instance will use it as a static IP. Otherwise it will try to request an IP from a DHCP server. cluster.nodes.worker.instances[*].mac string MAC used by the instance. If it is not set, it will be generated. cluster.nodes.worker.instances[*].ram number Overrides a default value for the RAM for that instance. cluster.nodes.worker.instances[*].cpu number Overrides a default value for that specific instance. cluster.nodes.worker.instances[*].mainDiskSize number Overrides a default value for that specific instance. cluster.nodes.worker.instances[*].dataDisks[*].name string Name of the additional data disk that is attached to the worker node. cluster.nodes.worker.instances[*].dataDisks[*].pool string Name of the data resource pool where the additional data disk is created. Referenced resource pool must be specified on the same host. cluster.nodes.worker.instances[*].dataDisks[*].size string Size of the additional data disk (in GiB) that is attached to the worker node.","title":"Cluster section"},{"location":"reference/reference/#kubernetes-section","text":"Name Type Default value Required? Description kubernetes.version string Yes Kubernetes version that will be installed. kubernetes.networkPlugin string calico Network plugin used within a Kubernetes cluster. Possible values are: flannel weave calico cilium canal kube-router kubernetes.dnsMode string coredns DNS server used within a Kubernetes cluster. Possible values are: coredns kubedns kubernetes.kubespray.url string https://github.com/kubernetes-sigs/kubespray.git URL to the Kubespray project. For example, it can be changed so that it targets your fork of a project. kubernetes.kubespray.version string Yes Kubespray version. Version is used to checkout into appropriate branch. kubernetes.kubespray.other.copyKubeconfig boolean false If set to true, Kubeconfig of a new cluster will be copied to the `~/.kube/admin.conf`. Please note that setting this to true can result in overwriting file on target location.","title":"Kubernetes section"},{"location":"user-guide/cluster-management/","text":"Cluster management Project currently supports the following actions that can be executed on the running Kubernetes cluster: scaling the cluster adding worker nodes, removing worker nodes, upgrading the cluster, destroying the cluster. Note Each action supports the --cluster <cluster_name> option, which allows you to execute the action on a specific cluster. By default, all actions are executed on the default cluster, which corresponds to using the --cluster default option. Export cluster configuration file Each action requires the cluster configuration file to be modified. Cluster configuration file can be exported using export command of the kubitect tool. kubitect export config > cluster.yaml Scale the cluster Add worker nodes to the cluster In the configuration file add new worker nodes to cluster.nodes.worker.instances list. cluster.yaml cluster : ... nodes : ... worker : instances : - id : 1 - id : 2 # New worker node - id : 3 # New worker node Apply the modified configuration using kubitect tool to add new worker nodes: kubitect apply --config cluster.yaml --action scale Remove worker nodes from the cluster In the configuration file remove worker nodes from cluster.nodes.worker.instances list. cluster.yaml cluster : ... nodes : ... worker : instances : - id : 1 #- id: 2 #- id: 3 Apply the modified configuration using kubitect tool to remove worker nodes: kubitect apply --config cluster.yaml --action scale Upgrade the cluster Important Do not skip releases when upgrading--upgrade by one tag at a time. For more information read Kubespray upgrades . In the cluster configuration file set the following variables: + kubernetes.version and + kubernetes.kubespray.version . Note Before upgrading the cluster, make sure that Kubespray supports a specific Kubernetes version. Example: cluster.yaml kubernetes : version : \"v1.22.5\" # Old value: \"v1.21.6\" ... kubespray : version : \"v2.18.0\" # Old value: \"v2.17.1\" ... Apply the modified configuration using kubitect tool: kubitect apply --config cluster.yaml --action upgrade Destroy the cluster To destroy the cluster, simply run: kubitect destroy","title":"Cluster management"},{"location":"user-guide/cluster-management/#export-cluster-configuration-file","text":"Each action requires the cluster configuration file to be modified. Cluster configuration file can be exported using export command of the kubitect tool. kubitect export config > cluster.yaml","title":"Export cluster configuration file"},{"location":"user-guide/cluster-management/#scale-the-cluster","text":"","title":"Scale the cluster"},{"location":"user-guide/cluster-management/#add-worker-nodes-to-the-cluster","text":"In the configuration file add new worker nodes to cluster.nodes.worker.instances list. cluster.yaml cluster : ... nodes : ... worker : instances : - id : 1 - id : 2 # New worker node - id : 3 # New worker node Apply the modified configuration using kubitect tool to add new worker nodes: kubitect apply --config cluster.yaml --action scale","title":"Add worker nodes to the cluster"},{"location":"user-guide/cluster-management/#remove-worker-nodes-from-the-cluster","text":"In the configuration file remove worker nodes from cluster.nodes.worker.instances list. cluster.yaml cluster : ... nodes : ... worker : instances : - id : 1 #- id: 2 #- id: 3 Apply the modified configuration using kubitect tool to remove worker nodes: kubitect apply --config cluster.yaml --action scale","title":"Remove worker nodes from the cluster"},{"location":"user-guide/cluster-management/#upgrade-the-cluster","text":"Important Do not skip releases when upgrading--upgrade by one tag at a time. For more information read Kubespray upgrades . In the cluster configuration file set the following variables: + kubernetes.version and + kubernetes.kubespray.version . Note Before upgrading the cluster, make sure that Kubespray supports a specific Kubernetes version. Example: cluster.yaml kubernetes : version : \"v1.22.5\" # Old value: \"v1.21.6\" ... kubespray : version : \"v2.18.0\" # Old value: \"v2.17.1\" ... Apply the modified configuration using kubitect tool: kubitect apply --config cluster.yaml --action upgrade","title":"Upgrade the cluster"},{"location":"user-guide/cluster-management/#destroy-the-cluster","text":"To destroy the cluster, simply run: kubitect destroy","title":"Destroy the cluster"},{"location":"user-guide/getting-started/","text":"Getting Started In this step-by-step guide, you will learn how to prepare a custom cluster configuration file and use it to create a functional Kubernetes cluster consisting of a single master node and three worker nodes. Note See reference documentation for explanations of each possible configuration property. Step 1 - Make sure all requirements are satisfied For the successful installation of the Kubernetes cluster, some requirements must be met. Step 2 - Create cluster configuration file In the quick start you have created a very basic Kubernetes cluster from predefined cluster configuration file. If configuration is not explicitly provided to the command-line tool using --config option, default cluster configuration file is used ( /examples/default-cluster.yaml ). Now it's time to create your own cluster topology. Before you begin create a new yaml file. touch kubitect.yaml Step 3 - Prepare hosts configuration In the cluster configuration file, we will first define hosts. Hosts represent target servers. A host can be either a local or a remote machine. Localhost Remote host If the cluster is set up on the same machine where the command line tool is installed, we specify a host whose connection type is set to local . hosts : - name : localhost # Can be anything connection : type : local When cluster is deployed on the remote machine, the IP address of the remote machine along with the SSH credentails needs to be specified for the host. hosts : - name : my-remote-host connection : type : remote user : myuser ip : 10.10.40.143 # IP address of the remote host ssh : keyfile : \"~/.ssh/id_rsa_server1\" # Password-less SSH key file In this tutorial we will use only localhost. Step 4 - Define cluster infrastructure The second part of the configuration file consists of the cluster infrastructure. In this part, all virtual machines are defined along with their properties such as operating system, CPU cores, amount of RAM and so on. Let's take a look at the following configuration: cluster : name : \"my-k8s-cluster\" network : ... nodeTemplate : ... nodes : ... We can see that the infrastructure configuration consists of the cluster name and 3 subsections: cluster.name is a cluster name that is used as a prefix for each resource created by kubitect . cluster.network holds information about the network properties of the cluster. cluster.nodeTemplate contains properties that apply to all our nodes. For example, properties like operating system, SSH user, and SSH private key are the same for all our nodes. cluster.nodes subsection defines each node in our cluster. Now that we have a general idea about the infrastructure configuration, we can look at each of these subsections in more detail. Step 4.1 - Cluster network The cluster network subsection defines the network that our cluster will use. Currently, two network modes are supported - NAT and bridge. The nat network mode instructs kubitect to create a virtual network that does network address translation. This mode allows us to use IP address ranges that do not exist within our local area network (LAN). The bridge network mode instructs kubitect to use a predefined bridge interface. In this mode, virtual machines can connect directly to LAN. Using this mode is mandatory when you set up a cluster that spreads over multipe hosts. To keep this tutorial as simple as possible, we will use the NAT mode, as it does not require a preconfigured bridge interface. cluster : ... network : mode : \"nat\" cidr : \"192.168.113.0/24\" The above configuration will instruct kubitect to create a virtual network that uses 192.168.113.0/24 IP range. Step 4.2 - Node template As mentioned earlier, the nodeTemplate subsection is used to define general properties of our nodes. Required properties are: user is the name of the user that will be created on all virtual machines and will also be used for SSH. image.distro defines the type of the used operating system (ubuntu, debian, ...). image.source defines the location of the OS image. It can be either a local file system path or an URL. Besides the required properties, there are two potentially useful properties: ssh.addToKnownHosts - if set to true, all virtual machines will be added to SSH known hosts. If you later destroy the cluster, these virtual machines will also be removed from the known hosts. updateOnBoot - if set to true, all virtual machines are updated at the first boot. Our noteTemplate subsection now looks like this: cluster : ... nodeTemplate : user : \"k8s\" ssh : addToKnownHosts : true image : distro : \"ubuntu\" source : \"https://cloud-images.ubuntu.com/releases/focal/release-20220111/ubuntu-20.04-server-cloudimg-amd64.img\" updateOnBoot : true Step 4.3 - Cluster nodes In the nodes subsection, we can define three types of nodes: loadBalancer nodes are internal load balancers used to expose the Kubernetes control plane at a single endpoint. master nodes are Kubernetes master nodes that also contain an etcd key-value store. Since etcd is present on these nodes, the number of master nodes must be odd. For more information, see etcd FAQ . worker nodes are the nodes on which your actual workload runs. In this tutorial, we will use only one master node, so internal load balancers are not required. The easiest way to explain this part is to look at the actual configuration: cluster : ... nodes : master : default : # Default properties of all master node instances ram : 4 cpu : 2 mainDiskSize : 32 instances : # Master node instances - id : 1 ip : 192.168.113.10 worker : default : ram : 4 cpu : 2 mainDiskSize : 32 instances : - id : 1 ip : 192.168.113.21 cpu : 4 # Override default vCPU value for this node ram : 8 # Override default amount of RAM for this node - id : 7 ip : 192.168.113.27 mac : \"52:54:00:00:00:42\" # Specify MAC address for this node - id : 99 # If ip property is omitted, node will request an IP address # from the DHCP server. If mac property is omitted, MAC address # will be auto generated. Step 4.4 - Kubernetes properties The last part of the cluster configuration consists of the Kubernetes properties. In this section we define the Kubernetes version, the DNS plugin and so on. It is also important to check if Kubespray supports a specific Kubernetes version. If you are using a custom Kubespray, you can also specify the URL to a custom Git repository. kubernetes : version : \"v1.22.6\" networkPlugin : \"calico\" dnsMode : \"coredns\" kubespray : version : \"v2.18.1\" # url: URL to custom Kubespray git repository. # (default is: https://github.com/kubernetes-sigs/kubespray.git) Step 5 - Create the cluster Our final cluster configuration looks like this: kubitect.yaml hosts : - name : localhost connection : type : local cluster : name : \"my-k8s-cluster\" network : mode : \"nat\" cidr : \"192.168.113.0/24\" nodeTemplate : user : \"k8s\" ssh : addToKnownHosts : true image : distro : \"ubuntu\" source : \"https://cloud-images.ubuntu.com/releases/focal/release-20220111/ubuntu-20.04-server-cloudimg-amd64.img\" updateOnBoot : true nodes : master : default : ram : 4 cpu : 2 mainDiskSize : 32 instances : - id : 1 ip : 192.168.113.10 worker : default : ram : 4 cpu : 2 mainDiskSize : 32 instances : - id : 1 ip : 192.168.113.21 cpu : 4 ram : 8 - id : 7 ip : 192.168.113.27 mac : \"52:54:00:00:00:42\" - id : 99 kubernetes : version : \"v1.22.6\" networkPlugin : \"calico\" dnsMode : \"coredns\" kubespray : version : \"v2.18.1\" Now create the cluster by applying your custom configuration using the kubitect command line tool. Also, let's name our cluster my-first-cluster . kubitect apply --cluster my-first-cluster --config kubitect.yaml Tip If you encounter any issues during the installation, please refer to the troubleshooting page first. When the cluster is applied, it is created in kubitect home directory, which has the following structure. ~/.kubitect \u251c\u2500\u2500 clusters \u2502 \u251c\u2500\u2500 default \u2502 \u251c\u2500\u2500 my-first-cluster \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 bin \u2514\u2500\u2500 ... All created clusters can be listed at any time using kubitect command line tool. kubitect list clusters Step 6 - Test the cluster After successful installation of the Kubernetes cluster, Kubeconfig is created in the cluster's directory. To export the Kubeconfig to a separate file, run the following command. kubitect export kubeconfig --cluster my-first-cluster > kubeconfig.yaml Use the exported Kubeconfig to list all cluster nodes. kubectl get nodes --kubeconfig kubeconfig.yaml Congratulations, you have completed the getting started tutorial.","title":"Getting started"},{"location":"user-guide/getting-started/#step-1-make-sure-all-requirements-are-satisfied","text":"For the successful installation of the Kubernetes cluster, some requirements must be met.","title":"Step 1 - Make sure all requirements are satisfied"},{"location":"user-guide/getting-started/#step-2-create-cluster-configuration-file","text":"In the quick start you have created a very basic Kubernetes cluster from predefined cluster configuration file. If configuration is not explicitly provided to the command-line tool using --config option, default cluster configuration file is used ( /examples/default-cluster.yaml ). Now it's time to create your own cluster topology. Before you begin create a new yaml file. touch kubitect.yaml","title":"Step 2 - Create cluster configuration file"},{"location":"user-guide/getting-started/#step-3-prepare-hosts-configuration","text":"In the cluster configuration file, we will first define hosts. Hosts represent target servers. A host can be either a local or a remote machine. Localhost Remote host If the cluster is set up on the same machine where the command line tool is installed, we specify a host whose connection type is set to local . hosts : - name : localhost # Can be anything connection : type : local When cluster is deployed on the remote machine, the IP address of the remote machine along with the SSH credentails needs to be specified for the host. hosts : - name : my-remote-host connection : type : remote user : myuser ip : 10.10.40.143 # IP address of the remote host ssh : keyfile : \"~/.ssh/id_rsa_server1\" # Password-less SSH key file In this tutorial we will use only localhost.","title":"Step 3 - Prepare hosts configuration"},{"location":"user-guide/getting-started/#step-4-define-cluster-infrastructure","text":"The second part of the configuration file consists of the cluster infrastructure. In this part, all virtual machines are defined along with their properties such as operating system, CPU cores, amount of RAM and so on. Let's take a look at the following configuration: cluster : name : \"my-k8s-cluster\" network : ... nodeTemplate : ... nodes : ... We can see that the infrastructure configuration consists of the cluster name and 3 subsections: cluster.name is a cluster name that is used as a prefix for each resource created by kubitect . cluster.network holds information about the network properties of the cluster. cluster.nodeTemplate contains properties that apply to all our nodes. For example, properties like operating system, SSH user, and SSH private key are the same for all our nodes. cluster.nodes subsection defines each node in our cluster. Now that we have a general idea about the infrastructure configuration, we can look at each of these subsections in more detail.","title":"Step 4 - Define cluster infrastructure"},{"location":"user-guide/getting-started/#step-41-cluster-network","text":"The cluster network subsection defines the network that our cluster will use. Currently, two network modes are supported - NAT and bridge. The nat network mode instructs kubitect to create a virtual network that does network address translation. This mode allows us to use IP address ranges that do not exist within our local area network (LAN). The bridge network mode instructs kubitect to use a predefined bridge interface. In this mode, virtual machines can connect directly to LAN. Using this mode is mandatory when you set up a cluster that spreads over multipe hosts. To keep this tutorial as simple as possible, we will use the NAT mode, as it does not require a preconfigured bridge interface. cluster : ... network : mode : \"nat\" cidr : \"192.168.113.0/24\" The above configuration will instruct kubitect to create a virtual network that uses 192.168.113.0/24 IP range.","title":"Step 4.1 - Cluster network"},{"location":"user-guide/getting-started/#step-42-node-template","text":"As mentioned earlier, the nodeTemplate subsection is used to define general properties of our nodes. Required properties are: user is the name of the user that will be created on all virtual machines and will also be used for SSH. image.distro defines the type of the used operating system (ubuntu, debian, ...). image.source defines the location of the OS image. It can be either a local file system path or an URL. Besides the required properties, there are two potentially useful properties: ssh.addToKnownHosts - if set to true, all virtual machines will be added to SSH known hosts. If you later destroy the cluster, these virtual machines will also be removed from the known hosts. updateOnBoot - if set to true, all virtual machines are updated at the first boot. Our noteTemplate subsection now looks like this: cluster : ... nodeTemplate : user : \"k8s\" ssh : addToKnownHosts : true image : distro : \"ubuntu\" source : \"https://cloud-images.ubuntu.com/releases/focal/release-20220111/ubuntu-20.04-server-cloudimg-amd64.img\" updateOnBoot : true","title":"Step 4.2 - Node template"},{"location":"user-guide/getting-started/#step-43-cluster-nodes","text":"In the nodes subsection, we can define three types of nodes: loadBalancer nodes are internal load balancers used to expose the Kubernetes control plane at a single endpoint. master nodes are Kubernetes master nodes that also contain an etcd key-value store. Since etcd is present on these nodes, the number of master nodes must be odd. For more information, see etcd FAQ . worker nodes are the nodes on which your actual workload runs. In this tutorial, we will use only one master node, so internal load balancers are not required. The easiest way to explain this part is to look at the actual configuration: cluster : ... nodes : master : default : # Default properties of all master node instances ram : 4 cpu : 2 mainDiskSize : 32 instances : # Master node instances - id : 1 ip : 192.168.113.10 worker : default : ram : 4 cpu : 2 mainDiskSize : 32 instances : - id : 1 ip : 192.168.113.21 cpu : 4 # Override default vCPU value for this node ram : 8 # Override default amount of RAM for this node - id : 7 ip : 192.168.113.27 mac : \"52:54:00:00:00:42\" # Specify MAC address for this node - id : 99 # If ip property is omitted, node will request an IP address # from the DHCP server. If mac property is omitted, MAC address # will be auto generated.","title":"Step 4.3 - Cluster nodes"},{"location":"user-guide/getting-started/#step-44-kubernetes-properties","text":"The last part of the cluster configuration consists of the Kubernetes properties. In this section we define the Kubernetes version, the DNS plugin and so on. It is also important to check if Kubespray supports a specific Kubernetes version. If you are using a custom Kubespray, you can also specify the URL to a custom Git repository. kubernetes : version : \"v1.22.6\" networkPlugin : \"calico\" dnsMode : \"coredns\" kubespray : version : \"v2.18.1\" # url: URL to custom Kubespray git repository. # (default is: https://github.com/kubernetes-sigs/kubespray.git)","title":"Step 4.4 - Kubernetes properties"},{"location":"user-guide/getting-started/#step-5-create-the-cluster","text":"Our final cluster configuration looks like this: kubitect.yaml hosts : - name : localhost connection : type : local cluster : name : \"my-k8s-cluster\" network : mode : \"nat\" cidr : \"192.168.113.0/24\" nodeTemplate : user : \"k8s\" ssh : addToKnownHosts : true image : distro : \"ubuntu\" source : \"https://cloud-images.ubuntu.com/releases/focal/release-20220111/ubuntu-20.04-server-cloudimg-amd64.img\" updateOnBoot : true nodes : master : default : ram : 4 cpu : 2 mainDiskSize : 32 instances : - id : 1 ip : 192.168.113.10 worker : default : ram : 4 cpu : 2 mainDiskSize : 32 instances : - id : 1 ip : 192.168.113.21 cpu : 4 ram : 8 - id : 7 ip : 192.168.113.27 mac : \"52:54:00:00:00:42\" - id : 99 kubernetes : version : \"v1.22.6\" networkPlugin : \"calico\" dnsMode : \"coredns\" kubespray : version : \"v2.18.1\" Now create the cluster by applying your custom configuration using the kubitect command line tool. Also, let's name our cluster my-first-cluster . kubitect apply --cluster my-first-cluster --config kubitect.yaml Tip If you encounter any issues during the installation, please refer to the troubleshooting page first. When the cluster is applied, it is created in kubitect home directory, which has the following structure. ~/.kubitect \u251c\u2500\u2500 clusters \u2502 \u251c\u2500\u2500 default \u2502 \u251c\u2500\u2500 my-first-cluster \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 bin \u2514\u2500\u2500 ... All created clusters can be listed at any time using kubitect command line tool. kubitect list clusters","title":"Step 5 - Create the cluster"},{"location":"user-guide/getting-started/#step-6-test-the-cluster","text":"After successful installation of the Kubernetes cluster, Kubeconfig is created in the cluster's directory. To export the Kubeconfig to a separate file, run the following command. kubitect export kubeconfig --cluster my-first-cluster > kubeconfig.yaml Use the exported Kubeconfig to list all cluster nodes. kubectl get nodes --kubeconfig kubeconfig.yaml Congratulations, you have completed the getting started tutorial.","title":"Step 6 - Test the cluster"},{"location":"user-guide/installation/","text":"Installation Before starting with installation, make sure you meet all the requirements . Install Kubitect CLI tool After all requirements are met, download the Kubitect command line tool. curl -o kubitect.tar.gz -L https://github.com/MusicDin/kubitect/releases/download/v2.0.7/kubitect-v2.0.7-linux-amd64.tar.gz Unpack tar.gz file. tar -xzf kubitect.tar.gz Install Kubitect command line tool by placing the Kubitect binary file in /usr/local/bin directory. sudo mv kubitect /usr/local/bin/ Verify the installation by checking the Kubitect version. kubitect --version # kubitect version v2.0.7 Tip If you are using Kubitect for the first time, we strongly recommend you to take a look at the getting started tutorial. Enable shell autocomplete For example, to enable automplete for bash , run the following command. echo 'source <(kubitect completion bash)' >> ~/.bashrc Then reload your shell.","title":"Installation"},{"location":"user-guide/installation/#install-kubitect-cli-tool","text":"After all requirements are met, download the Kubitect command line tool. curl -o kubitect.tar.gz -L https://github.com/MusicDin/kubitect/releases/download/v2.0.7/kubitect-v2.0.7-linux-amd64.tar.gz Unpack tar.gz file. tar -xzf kubitect.tar.gz Install Kubitect command line tool by placing the Kubitect binary file in /usr/local/bin directory. sudo mv kubitect /usr/local/bin/ Verify the installation by checking the Kubitect version. kubitect --version # kubitect version v2.0.7 Tip If you are using Kubitect for the first time, we strongly recommend you to take a look at the getting started tutorial.","title":"Install Kubitect CLI tool"},{"location":"user-guide/installation/#enable-shell-autocomplete","text":"For example, to enable automplete for bash , run the following command. echo 'source <(kubitect completion bash)' >> ~/.bashrc Then reload your shell.","title":"Enable shell autocomplete"},{"location":"user-guide/load-balancer/","text":"Internal load balancing (iLB) Multiple master nodes ensure that services remain available if one or even more master nodes fail. Cluster has to be set up with an odd number of master nodes so that the quorum (the majority of master nodes) can be maintained if one or more masters fail. In the high-availability (HA) scenario, Kubernetes maintains a copy of the etcd databases on each master node, but holds elections for the kube-controller and kube-scheduler managers to avoid conflicts. This allows worker nodes to communicate with any master node through a single endpoint provided by load balancers. Configure HAProxy load balancers Specify load balancer instances in the cluster configuration file. cluster : ... nodes : ... loadBalancer : vip : 10.10.64.200 # Floating IP that should not be taken by any other device instances : - id : 1 - id : 40 Note: Load balancers id must be a number between 0 and 200, because their fail-over priority is calculated from the id . Cluster without internal load balancers If you have only one master node, the internal load balancers are redundant. In this case, remove (or comment out) all load balancer instances from the cluster configuration file: cluster : ... nodes : ... loadBalancer : ... #instances : #- id: 1 #- id: 40 Note: If multiple master nodes are specified, the IP of the first one is used as the cluster IP.","title":"Load balancer"},{"location":"user-guide/load-balancer/#configure-haproxy-load-balancers","text":"Specify load balancer instances in the cluster configuration file. cluster : ... nodes : ... loadBalancer : vip : 10.10.64.200 # Floating IP that should not be taken by any other device instances : - id : 1 - id : 40 Note: Load balancers id must be a number between 0 and 200, because their fail-over priority is calculated from the id .","title":"Configure HAProxy load balancers"},{"location":"user-guide/load-balancer/#cluster-without-internal-load-balancers","text":"If you have only one master node, the internal load balancers are redundant. In this case, remove (or comment out) all load balancer instances from the cluster configuration file: cluster : ... nodes : ... loadBalancer : ... #instances : #- id: 1 #- id: 40 Note: If multiple master nodes are specified, the IP of the first one is used as the cluster IP.","title":"Cluster without internal load balancers"},{"location":"user-guide/local-development/","text":"Local development This document shows how to build a CLI tool manually and how to use the project without creating any files outside the project's directory. Clone the project First, you have to clone the project. git clone https://github.com/MusicDin/kubitect Afterwards, move into the cloned project. cd kubitect Install Kubitect CLI tool Kubitect CLI tool is implemented in Go using cobra library. The tool can either be installed from already built versions available on GitHub or you can build it manually. To manually build the CLI tool, first change to the cli directory. cd cli Now, using build the tool using go. go build . This will create a cli binary file, which can be moved into /usr/local/bin/ directory to use it globaly. sudo mv cli /usr/bin/local/kubitect Local development By default, Kubitect creates and manages clusters located in the Kubitect home directory ( ~/.kubitect ). Although this pattern is very useful for everyday use, it can be somewhat inconvenient if you are actively making changes to the project, as each change must be committed to the Git repository. For this very reason, the Kubitect CLI tool has the --local option, which replaces the project's home directory with the path of the current directory. This way, the source code from the current directory is used to create a cluster and all cluster-related files are created in the current directory. This option can be used with most actions, such as apply or destroy . kubitect apply --local","title":"Local development"},{"location":"user-guide/local-development/#clone-the-project","text":"First, you have to clone the project. git clone https://github.com/MusicDin/kubitect Afterwards, move into the cloned project. cd kubitect","title":"Clone the project"},{"location":"user-guide/local-development/#install-kubitect-cli-tool","text":"Kubitect CLI tool is implemented in Go using cobra library. The tool can either be installed from already built versions available on GitHub or you can build it manually. To manually build the CLI tool, first change to the cli directory. cd cli Now, using build the tool using go. go build . This will create a cli binary file, which can be moved into /usr/local/bin/ directory to use it globaly. sudo mv cli /usr/bin/local/kubitect","title":"Install Kubitect CLI tool"},{"location":"user-guide/local-development/#local-development","text":"By default, Kubitect creates and manages clusters located in the Kubitect home directory ( ~/.kubitect ). Although this pattern is very useful for everyday use, it can be somewhat inconvenient if you are actively making changes to the project, as each change must be committed to the Git repository. For this very reason, the Kubitect CLI tool has the --local option, which replaces the project's home directory with the path of the current directory. This way, the source code from the current directory is used to create a cluster and all cluster-related files are created in the current directory. This option can be used with most actions, such as apply or destroy . kubitect apply --local","title":"Local development"},{"location":"user-guide/overview/","text":"","title":"Overview"},{"location":"user-guide/quick-start/","text":"Quick start Step 1 - Create the cluster Run the following command to create the default cluster. Cluster will be created in ~/.kubitect/clusters/default/ directory. kubitect apply Note Using a --cluster option, you can provide custom cluster name. This way multiple clusters can be created. Step 2 - Export kubeconfig After successful installation of the Kubernetes cluster, Kubeconfig will be created within cluster's directory. To export the Kubeconfig into custom file run the following command. kubitect export kubeconfig > kubeconfig.yaml Step 3 - Test the cluster Test if the cluster works by displaying all cluster nodes. kubectl get nodes --kubeconfig kubeconfig.yaml","title":"Quick start"},{"location":"user-guide/quick-start/#step-1-create-the-cluster","text":"Run the following command to create the default cluster. Cluster will be created in ~/.kubitect/clusters/default/ directory. kubitect apply Note Using a --cluster option, you can provide custom cluster name. This way multiple clusters can be created.","title":"Step 1 - Create the cluster"},{"location":"user-guide/quick-start/#step-2-export-kubeconfig","text":"After successful installation of the Kubernetes cluster, Kubeconfig will be created within cluster's directory. To export the Kubeconfig into custom file run the following command. kubitect export kubeconfig > kubeconfig.yaml","title":"Step 2 - Export kubeconfig"},{"location":"user-guide/quick-start/#step-3-test-the-cluster","text":"Test if the cluster works by displaying all cluster nodes. kubectl get nodes --kubeconfig kubeconfig.yaml","title":"Step 3 - Test the cluster"},{"location":"user-guide/requirements/","text":"Requirements Local machine On the machine where the command-line tool is installed, the following requirements must be met: Git Python >= 3.0 Python virtualenv Hosts A host is a physical server that can be either a local or remote machine. Each host must have: installed hypervisor and installed libvirt virtualization API If the host is a remote machine, a local machine must have: appropriate pasword-less SSH keys to sucessfully connect to the remote hypervisor. Example - Install KVM For example, to install the KVM (Kernel Virtual Machine) hypervisor and libvirt, use yum or apt to install the following packages: qemu qemu-kvm libvirt-clients libvirt-daemon libvirt-daemon-system After installation, also add user to the kvm and libvirt groups.","title":"Requirements"},{"location":"user-guide/requirements/#local-machine","text":"On the machine where the command-line tool is installed, the following requirements must be met: Git Python >= 3.0 Python virtualenv","title":"Local machine"},{"location":"user-guide/requirements/#hosts","text":"A host is a physical server that can be either a local or remote machine. Each host must have: installed hypervisor and installed libvirt virtualization API If the host is a remote machine, a local machine must have: appropriate pasword-less SSH keys to sucessfully connect to the remote hypervisor.","title":"Hosts"},{"location":"user-guide/requirements/#example-install-kvm","text":"For example, to install the KVM (Kernel Virtual Machine) hypervisor and libvirt, use yum or apt to install the following packages: qemu qemu-kvm libvirt-clients libvirt-daemon libvirt-daemon-system After installation, also add user to the kvm and libvirt groups.","title":"Example - Install KVM"},{"location":"user-guide/troubleshooting/","text":"Troubleshooting Is your problem not listed here? If the troubleshooting page is missing an error you encountered, please report it on GitHub by opening an issue . By doing so, you will help improve the project and help others find the solution to the same problem faster. General errors Virtualenv not found Error Explanation Solution Error Output: /bin/sh: 1: virtualenv: not found /bin/sh: 2: ansible-playbook: not found Explanation The error indicates that the virtualenv is not installed. Solution There are many ways to install virtualenv . For all installation options you can refere to their official documentation - Virtualenv installation . For example, virtualenv can be installed using pip . First install pip. sudo apt install python3-pip Then install virtualenv using pip3. pip3 install virtualenv KVM/Libvirt errors Failed to connect socket (No such file or directory) Error Explanation Solution Error Error: virError(Code=38, Domain=7, Message='Failed to connect socket to '/var/run/libvirt/libvirt-sock': No such file or directory') Explanation The problem may occur when libvirt is not started. Solution Make sure that the libvirt service is running: sudo systemctl status libvirtd If the libvirt service is not running, start it: sudo systemctl start libvirtd Optional: Start the libvirt service automatically at boot time: sudo systemctl enable libvirtd Failed to connect socket (Permission denied) Error Explanation Solution Error Error: virError(Code=38, Domain=7, Message='Failed to connect socket to '/var/run/libvirt/libvirt-sock': Permission denied') Explanation The error indicates that either the libvirtd service is not running or the current user is not in the libvirt (or kvm ) group. Solution If the libvirtd service is not running, start it: sudo systemctl start libvirtd Add the current user to the libvirt and kvm groups if needed: # Add current user to groups sudo usermod -aG libvirt,kvm ` id -un ` # Verify groups are added id -nG # Reload user session su - ` id -un ` Error creating libvirt domain Error Explanation Solution Error Error: Error creating libvirt domain: \u2026 Could not open '/tmp/terraform_libvirt_provider_images/image.qcow2': Permission denied') Explanation The error indicates that the file cannot be created in the specified location due to missing permissions. Make sure the directory exists. Make sure the directory of the file that is being denied has appropriate user permissions. Optionally qemu security driver can be disabled. Solution Make sure the security_driver in /etc/libvirt/qemu.conf is set to none instead of selinux . This line is commented out by default, so you should uncomment it if needed: # /etc/libvirt/qemu.conf ... security_driver = \"none\" ... Do not forget to restart the libvirt service after making the changes: sudo systemctl restart libvirtd Libvirt domain already exists Error Explanation Solution Error Error: Error defining libvirt domain: virError(Code=9, Domain=20, Message='operation failed: domain ' your-domain ' already exists with uuid '...') Explanation The error indicates that the libvirt domain (virtual machine) already exists. Solution The resource you are trying to create already exists. Make sure you destroy the resource: virsh destroy your-domain virsh undefine your-domain You can verify that the domain was successfully removed: virsh dominfo --domain your-domain If the domain was successfully removed, the output should look something like this: error: failed to get domain ' your-domain ' Libvirt volume already exists Error Explanation Solution Error Error: Error creating libvirt volume: virError(Code=90, Domain=18, Message='storage volume ' your-volume .qcow2' exists already') and / or Error:Error creating libvirt volume for cloudinit device cloud-init .iso: virError(Code=90, Domain=18, Message='storage volume ' cloud-init .iso' exists already') Explanation The error indicates that the specified volume already exists. Solution Volumes created by Libvirt are still attached to the images, which prevents a new volume from being created with the same name. Therefore, these volumes must be removed: virsh vol-delete cloud-init .iso --pool your_resource_pool and / or virsh vol-delete your-volume .qcow2 --pool your_resource_pool Libvirt storage pool already exists Error Explanation Solution Error Error: Error storage pool ' your-pool ' already exists Explanation The error indicates that the libvirt storage pool already exists. Solution Remove the existing libvirt storage pool. virsh pool-destroy your-pool && virsh pool-undefine your-pool Failed to apply firewall rules Error Explanation Solution Error Error: internal error: Failed to apply firewall rules /sbin/iptables -w --table filter --insert LIBVIRT_INP --in-interface virbr2 --protocol tcp --destination-port 67 --jump ACCEPT: iptables: No chain/target/match by that name. Explanation Libvirt was already running when firewall (usually FirewallD) was started/installed. Therefore, libvirtd service must be restarted to detect the changes. Solution Restart the libvirtd service: sudo systemctl restart libvirtd Failed to remove storage pool Error Explanation Solution Error Error: error deleting storage pool: failed to remove pool '/var/lib/libvirt/pools/local-k8s-cluster-main-resource-pool': Directory not empty Explanation The pool cannot be deleted because there are still some volumes in the pool. Therefore, the volumes should be removed before the pool can be deleted. Solution Make sure the pool is running. virsh pool-start --pool local-k8s-cluster-main-resource-pool List volumes in the pool. virsh vol-list --pool local-k8s-cluster-main-resource-pool # Name Path # ------------------------------------------------------------------------------------- # base_volume /var/lib/libvirt/pools/local-k8s-cluster-main-resource-pool/base_volume Delete listed volumes from the pool. virsh vol-delete --pool local-k8s-cluster-main-resource-pool --vol base_volume Destroy and undefine the pool. virsh pool-destroy --pool local-k8s-cluster-main-resource-pool virsh pool-undefine --pool local-k8s-cluster-main-resource-pool HAProxy load balancer errors Random HAProxy (503) bad gateway Error Explanation Solution Error HAProxy returns a random HTTP 503 (Bad gateway) error. Explanation More than one HAProxy processes are listening on the same port. Solution 1 For example, if an error is thrown when accessing port 80 , check which processes are listening on port 80 on the load balancer VM: netstat -lnput | grep 80 # Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name # tcp 0 0 192.168.113.200:80 0.0.0.0:* LISTEN 1976/haproxy # tcp 0 0 192.168.113.200:80 0.0.0.0:* LISTEN 1897/haproxy If you see more than one process, kill the unnecessary process: kill 1976 Note: You can kill all HAProxy processes and only one will be automatically recreated. Solution 2 Check the HAProxy configuration file ( config/haproxy/haproxy.cfg ) that it does not contain 2 frontends bound to the same port.","title":"Troubleshooting"},{"location":"user-guide/troubleshooting/#general-errors","text":"","title":"General errors"},{"location":"user-guide/troubleshooting/#virtualenv-not-found","text":"Error Explanation Solution Error Output: /bin/sh: 1: virtualenv: not found /bin/sh: 2: ansible-playbook: not found Explanation The error indicates that the virtualenv is not installed. Solution There are many ways to install virtualenv . For all installation options you can refere to their official documentation - Virtualenv installation . For example, virtualenv can be installed using pip . First install pip. sudo apt install python3-pip Then install virtualenv using pip3. pip3 install virtualenv","title":"Virtualenv not found"},{"location":"user-guide/troubleshooting/#kvmlibvirt-errors","text":"","title":"KVM/Libvirt errors"},{"location":"user-guide/troubleshooting/#failed-to-connect-socket-no-such-file-or-directory","text":"Error Explanation Solution Error Error: virError(Code=38, Domain=7, Message='Failed to connect socket to '/var/run/libvirt/libvirt-sock': No such file or directory') Explanation The problem may occur when libvirt is not started. Solution Make sure that the libvirt service is running: sudo systemctl status libvirtd If the libvirt service is not running, start it: sudo systemctl start libvirtd Optional: Start the libvirt service automatically at boot time: sudo systemctl enable libvirtd","title":"Failed to connect socket (No such file or directory)"},{"location":"user-guide/troubleshooting/#failed-to-connect-socket-permission-denied","text":"Error Explanation Solution Error Error: virError(Code=38, Domain=7, Message='Failed to connect socket to '/var/run/libvirt/libvirt-sock': Permission denied') Explanation The error indicates that either the libvirtd service is not running or the current user is not in the libvirt (or kvm ) group. Solution If the libvirtd service is not running, start it: sudo systemctl start libvirtd Add the current user to the libvirt and kvm groups if needed: # Add current user to groups sudo usermod -aG libvirt,kvm ` id -un ` # Verify groups are added id -nG # Reload user session su - ` id -un `","title":"Failed to connect socket (Permission denied)"},{"location":"user-guide/troubleshooting/#error-creating-libvirt-domain","text":"Error Explanation Solution Error Error: Error creating libvirt domain: \u2026 Could not open '/tmp/terraform_libvirt_provider_images/image.qcow2': Permission denied') Explanation The error indicates that the file cannot be created in the specified location due to missing permissions. Make sure the directory exists. Make sure the directory of the file that is being denied has appropriate user permissions. Optionally qemu security driver can be disabled. Solution Make sure the security_driver in /etc/libvirt/qemu.conf is set to none instead of selinux . This line is commented out by default, so you should uncomment it if needed: # /etc/libvirt/qemu.conf ... security_driver = \"none\" ... Do not forget to restart the libvirt service after making the changes: sudo systemctl restart libvirtd","title":"Error creating libvirt domain"},{"location":"user-guide/troubleshooting/#libvirt-domain-already-exists","text":"Error Explanation Solution Error Error: Error defining libvirt domain: virError(Code=9, Domain=20, Message='operation failed: domain ' your-domain ' already exists with uuid '...') Explanation The error indicates that the libvirt domain (virtual machine) already exists. Solution The resource you are trying to create already exists. Make sure you destroy the resource: virsh destroy your-domain virsh undefine your-domain You can verify that the domain was successfully removed: virsh dominfo --domain your-domain If the domain was successfully removed, the output should look something like this: error: failed to get domain ' your-domain '","title":"Libvirt domain already exists"},{"location":"user-guide/troubleshooting/#libvirt-volume-already-exists","text":"Error Explanation Solution Error Error: Error creating libvirt volume: virError(Code=90, Domain=18, Message='storage volume ' your-volume .qcow2' exists already') and / or Error:Error creating libvirt volume for cloudinit device cloud-init .iso: virError(Code=90, Domain=18, Message='storage volume ' cloud-init .iso' exists already') Explanation The error indicates that the specified volume already exists. Solution Volumes created by Libvirt are still attached to the images, which prevents a new volume from being created with the same name. Therefore, these volumes must be removed: virsh vol-delete cloud-init .iso --pool your_resource_pool and / or virsh vol-delete your-volume .qcow2 --pool your_resource_pool","title":"Libvirt volume already exists"},{"location":"user-guide/troubleshooting/#libvirt-storage-pool-already-exists","text":"Error Explanation Solution Error Error: Error storage pool ' your-pool ' already exists Explanation The error indicates that the libvirt storage pool already exists. Solution Remove the existing libvirt storage pool. virsh pool-destroy your-pool && virsh pool-undefine your-pool","title":"Libvirt storage pool already exists"},{"location":"user-guide/troubleshooting/#failed-to-apply-firewall-rules","text":"Error Explanation Solution Error Error: internal error: Failed to apply firewall rules /sbin/iptables -w --table filter --insert LIBVIRT_INP --in-interface virbr2 --protocol tcp --destination-port 67 --jump ACCEPT: iptables: No chain/target/match by that name. Explanation Libvirt was already running when firewall (usually FirewallD) was started/installed. Therefore, libvirtd service must be restarted to detect the changes. Solution Restart the libvirtd service: sudo systemctl restart libvirtd","title":"Failed to apply firewall rules"},{"location":"user-guide/troubleshooting/#failed-to-remove-storage-pool","text":"Error Explanation Solution Error Error: error deleting storage pool: failed to remove pool '/var/lib/libvirt/pools/local-k8s-cluster-main-resource-pool': Directory not empty Explanation The pool cannot be deleted because there are still some volumes in the pool. Therefore, the volumes should be removed before the pool can be deleted. Solution Make sure the pool is running. virsh pool-start --pool local-k8s-cluster-main-resource-pool List volumes in the pool. virsh vol-list --pool local-k8s-cluster-main-resource-pool # Name Path # ------------------------------------------------------------------------------------- # base_volume /var/lib/libvirt/pools/local-k8s-cluster-main-resource-pool/base_volume Delete listed volumes from the pool. virsh vol-delete --pool local-k8s-cluster-main-resource-pool --vol base_volume Destroy and undefine the pool. virsh pool-destroy --pool local-k8s-cluster-main-resource-pool virsh pool-undefine --pool local-k8s-cluster-main-resource-pool","title":"Failed to remove storage pool"},{"location":"user-guide/troubleshooting/#haproxy-load-balancer-errors","text":"","title":"HAProxy load balancer errors"},{"location":"user-guide/troubleshooting/#random-haproxy-503-bad-gateway","text":"Error Explanation Solution Error HAProxy returns a random HTTP 503 (Bad gateway) error. Explanation More than one HAProxy processes are listening on the same port. Solution 1 For example, if an error is thrown when accessing port 80 , check which processes are listening on port 80 on the load balancer VM: netstat -lnput | grep 80 # Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name # tcp 0 0 192.168.113.200:80 0.0.0.0:* LISTEN 1976/haproxy # tcp 0 0 192.168.113.200:80 0.0.0.0:* LISTEN 1897/haproxy If you see more than one process, kill the unnecessary process: kill 1976 Note: You can kill all HAProxy processes and only one will be automatically recreated. Solution 2 Check the HAProxy configuration file ( config/haproxy/haproxy.cfg ) that it does not contain 2 frontends bound to the same port.","title":"Random HAProxy (503) bad gateway"}]}